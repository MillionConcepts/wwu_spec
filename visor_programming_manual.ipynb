{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISOR admin programming manual and recipes\n",
    "----\n",
    "###  michael st. clair \n",
    "### v0.3a -- 2025-02-21\n",
    "\n",
    "This Jupyter Notebook discusses the structure of the VISOR backend and methods for modifying its contents using Python scripts. The code cells in this notebook\n",
    "are intended both as illustrative examples and as useful 'recipes' that\n",
    "can be minimally modified and incorporated into other notebooks or scripts to perform\n",
    "administrative tasks.\n",
    "\n",
    "*This is a preliminary version of this document intended for internal\n",
    "operations. Please do not publicly distribute.*\n",
    "\n",
    "### usage notes\n",
    "----\n",
    "* This notebook should always be launched using ```python manage.py shell_plus --notebook```. Otherwise, django won't get to run its setup scripts and you will get errors when you try to import models or alter the database. \n",
    "* many of these cells create huge amounts of output. to shrink this if you're tired of looking at it, double-click on the gutter to the left of the cell. to totally erase it, go up to the 'cell' menu and choose 'current outputs -> clear', or 'all outputs' -> clear to get rid of output for every cell.\n",
    "* this notebook assumes that it's being run in the root directory of the application. If you move it somewhere else or use the code snippets in files located in other places, you may have to adjust paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports\n",
    "----\n",
    "Run this next cell if you want the code to function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from functools import partial, reduce\n",
    "import json\n",
    "from operator import or_\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "import django\n",
    "from django.conf import settings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "django.setup()\n",
    "\n",
    "from recipes import samples\n",
    "from visor.dj_utils import are_in, djget, eta, fields\n",
    "from visor.io.handlers import ingest_sample_csv\n",
    "from visor.models import Database, Library, Sample\n",
    "from visor.spectral import make_filterset\n",
    "\n",
    "# the examples in this notebook don't do risky async stuff to the database.\n",
    "# they all work the exact same way on the backend as the admin console. however, \n",
    "# ipython/jupyter wraps itself in an event loop that looks scary to django. this \n",
    "# environment variable tells django to calm down about it.\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I: database structure\n",
    "----\n",
    "\n",
    "## I.1: basics and location\n",
    "\n",
    "VISOR is backed by a SQLite database. This database is entirely contained\n",
    "in one file: db.sqlite3. **Keep several backups of this file outside of the\n",
    "working tree of the application.** This lets you freely experiment with the\n",
    "database. If you do anything horrible to it, you can immediately repair it by\n",
    "overwriting the file in the application directory with one of these backups. The only entry-specific items that are not stored in this database are image files (links to the images are stored, but not files themselves, because filesystems are better than databases at storing files).\n",
    "\n",
    "### II.1.a: working live or offline\n",
    "\n",
    "Because it's easy to copy the database file, you can also work in a separate development directory, or on a totally different machine with a version of the software running locally. On the other hand, because it's easy to fix mistakes, it's also pretty safe to work on the live version. You can even swap the database file out while the application is running. Users will only notice if they make queries while the file is in the middle of being overwritten. \n",
    "\n",
    "## I.2: django and models\n",
    "\n",
    "VISOR primarily uses the Python framework Django to interact with the\n",
    "database. Django calls SQL tables -- which are essentially big spreadsheets\n",
    "stored inside the database -- \"models\".\n",
    "There are five important models / tables / spreadsheets in the VISOR proper:\n",
    "\n",
    "* ```Sample``` (individual samples)\n",
    "* ```Database``` (origin databases, like ASTER/ECOSTRESS)\n",
    "* ```FilterSet``` (definitions for sets of filters, like Mastcam-Z's, used\n",
    "    for generating simulated reflectance curves)\n",
    "* ```Library``` (application- or team-specific groups of samples, and maybe\n",
    "    other things later -- this is fully functional, but not currently populated)\n",
    "* ```SampleType``` (top-level physical categories of sample, like minerals or\n",
    "    coatings -- this is again fully functional, but only skeletally populated)\n",
    "\n",
    "*Note: while you probably don't want to interact with them from the Python\n",
    "shell, admin tables, including users and their access information, are also\n",
    "stored in the database. So, for instance, if you roll back to an earlier\n",
    "version of the database after changing a user's password but before making a\n",
    "new database backup, that password will be reset to the earlier version.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. searching the database\n",
    "----\n",
    "\n",
    "## II.1: searching the database\n",
    "\n",
    "VISOR includes some helper functions that make Django easier to use. By default, Django uses \"querysets\" to interact with tables. Querysets are powerful, but use a custom syntax that combines SQL queries and Python. This syntax is sometimes awkward and rarely looks like idiomatic Python. This section shows how to define search functions that are quicker to learn and use.\n",
    "\n",
    "### II.1.a: custom search functions\n",
    "\n",
    "The next cell defines a simple search function ```samples``` that looks for\n",
    "samples that contain a particular value anywhere in a particular field, case-insensitive.\n",
    " \n",
    "*Note: ```samples``` is also included in the recipes.py module, but\n",
    "manipulating the code in the cell below will allow you to define different versions of\n",
    "it.*\n",
    "\n",
    "The syntax is simply: ```samples(value, field)```; it returns a ```QuerySet```\n",
    "(which can mostly be treated as a Python list) of all samples containing that value in that field.\n",
    "\n",
    "Other useful values for ```querytype``` in cousins of ```samples``` include\n",
    "'lt' or 'gt' (less/greater than) or 'iexact' (exact match). dropping the\n",
    "leading 'i' makes the search case-sensitive.\n",
    "\n",
    "The names VISOR prints for samples in Python shell / notebook are formatted like this:\n",
    "\n",
    "```sample name + _ + sample id (in database of origin) + _ + database-of-origin short name```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define partially-evaluated convenience function\n",
    "get_contains = partial(\n",
    "    djget, \n",
    "    model=Sample, \n",
    "    value = \"\",\n",
    "    # the field value is model-specific! you can omit it if you don't want to\n",
    "    # use the shortened call types discussed in II.1.c\n",
    "    field = \"sample_name\", \n",
    "    querytype='icontains'\n",
    ")\n",
    "# reorder arguments to prevent collisions\n",
    "samples = eta(get_contains, \"value\", \"field\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1.b: fetch all hematites in the database and look at 5 of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: the vanilla django equivalent to the next line is: \n",
    "# hematites = Sample.objects.filter(sample_name__icontains='hematite')\n",
    "hematites = samples('hematite', 'sample_name')\n",
    "random.choices(hematites, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1.c: check total number of samples, or of a subset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# samples() looks in the sample_name field by default.\n",
    "# called with no arguments, it returns all values in the model.\n",
    "# note: the vanilla django way to do that is to call Sample.objects.all().\n",
    "\n",
    "len(samples(\"smectite\")), len(samples()), len(Sample.objects.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.2: fields and values of models\n",
    "\n",
    "There are lots of ways to get fields and field values from the database. See the next few cells for some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2.a: get every field of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fields(Sample), fields(Database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2.b: get values of a particular field from instances of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_sample = random.choice(samples())\n",
    "print(random_sample.sample_name)\n",
    "smectites = samples(\"smectite\")\n",
    "print([\n",
    "    sample.id for sample in smectites\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2.c: get unique values of a field, ordered alphabetically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "names = [\n",
    "    name_list[0] for name_list in\n",
    "    set(samples().values_list('grain_size'))\n",
    "]\n",
    "names.sort()\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## II.3 related model fields\n",
    "\n",
    "Accessing fields from a different model is done by, depending on context:\n",
    "* using chained dots (like: ```model.other_model.field```)\n",
    "* separating the related field name and the field you want from the\n",
    "    other model by a double underscore (like: ```sc(value, \"other_model__field\")```)\n",
    "\n",
    "### II.3.a: learn about a sample's database of origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_sample = random.choice(samples())\n",
    "print(random_sample.origin.name) # full name of that sample's database of origin\n",
    "print(random_sample.origin.url) # url for that sample's database of origin\n",
    "# is that sample in the group of all samples whose databases of origin have that \n",
    "# full name? (hopefully yes, or something is very wrong) \n",
    "print(random_sample in samples(random_sample.origin.name, \"origin__name\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.4 interpreting Sample model fields\n",
    "\n",
    "There are a *lot* of fields on the ```Sample``` model, and most of them are \n",
    "empty for most samples. This is because the table is intended to support\n",
    "content ingested from a bunch of different databases,  each of which has its\n",
    "own metadata standard. So, for instance, while we'd like to retain information\n",
    "about resolution if it's available in an input database, most of our input\n",
    "databases don't provide\n",
    "resolution values in their metadata. The fields you can expect to be on every\n",
    "or almost every sample are:\n",
    "* sample_name (Name of the sample from the original database, like \"Talc\")\n",
    "* sample_id (ID of the sample from the original database, retained for\n",
    "traceability)\n",
    "* id (unique ID number in VISOR, also known as a database primary key or PK)\n",
    "    * bear in mind that because a primary key is how a database distinguishes\n",
    "    objects, changing a sample's id field makes it a whole new entry\n",
    "* date_added (last modification date of the sample)\n",
    "* min_reflectance (minimum wavelength in the reflectance array)\n",
    "* max_reflectance (maximum wavelength in the reflectance array)\n",
    "* origin (database of origin -- this is an instance of the ```Database```\n",
    "    model)\n",
    "* released (has the sample been released to the public?)\n",
    "* reflectance (reflectance array flattened into a simple string) \n",
    "* simulated_spectra (dictionary of ```pandas DataFrames``` giving simulated\n",
    "    reflectance arrays flattened into a json string)\n",
    "\n",
    "### II.4.a: get a random sample and look at all its fields\n",
    "\n",
    "You can use the ```as_dict()``` method of a ```Sample``` object to get most\n",
    "things about it in a ```dict``` -- note that the flattened reflectance and\n",
    "simulated_spectra fields aren't very readable! See the next few cells for\n",
    " ways to interpret them as ```numpy``` arrays and ```pandas``` dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_sample = random.choice(samples())\n",
    "random_sample.as_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.4.b: look at properties of that sample's reflectance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reflectance = np.array(literal_eval(random_sample.reflectance))\n",
    "print(reflectance[0:10, 0]) # first 10 wavelength values of spectrum\n",
    "print(np.median(reflectance,0))  # median wavelength and reflectance of spectrum \n",
    "print(reflectance[:,1].mean()) # mean reflectance of spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.4.c: look at a simulated spectrum for that sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sim_zcam = pd.DataFrame(\n",
    "    json.loads(literal_eval(random_sample.simulated_spectra)['Mastcam-Z'])\n",
    ")\n",
    "sim_zcam # dataframe containing simulated values for Mastcam-Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III: manipulating database entries\n",
    "----\n",
    "## III.1: field assignment and model entry updates\n",
    "\n",
    "Similar methods can be used to modify entries in the database. The easiest way\n",
    "is to assign values directly to fields of a model instance (like an individual sample). This is useful if\n",
    "you want to quickly modify items without using the admin console. The single exception to this in \n",
    "**Important:** updating the fields of a model instance in memory **does not**\n",
    "automatically change it in the database. After\n",
    "modifying a model instance, calling its ```clean()``` and ```save()``` methods\n",
    "will validate its changed data and record the updated version in the database.\n",
    "Some other stuff only happens after you call ```save()```, generally things that require comparisons with other values in the database. For instance,\n",
    "simulated spectra are generated at that point for samples, and if a model\n",
    "instance doesn't have an id / primary key, it gets assigned one.\n",
    "\n",
    "### III.1.a: change an in-memory sample without saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample = samples()[0]\n",
    "sample.sample_name = sample.sample_name + \"_TEST\"\n",
    "print(sample.sample_name) # great! working great, right? the sample is updated!\n",
    "sample = samples()[0]\n",
    "print(sample.sample_name) # aww...no, the sample wasn't updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.1.b: make a test version of a sample and save it in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample = samples()[0]\n",
    "# remember that changing id / primary key makes something a \"new\" object from the \n",
    "# database's perspective; delete id so we don't overwrite the real sample\n",
    "sample.id = None\n",
    "# change its sample_name and sample_id fields to distinguish it\n",
    "sample.sample_name = sample.sample_name + \"_TEST\"\n",
    "sample.sample_id = sample.sample_id + \"_TEST\" \n",
    "sample.released = False # don't show visitors our silly test sample\n",
    "sample.clean() # validate sample fields\n",
    "sample.save() # save it in the database\n",
    "# is it there, and different from the original? hopefully.\n",
    "samples()[0], samples(sample.id, \"id\")[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.2.c: modify and save the test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# saving a model instance _without_ changing its id modifies the existing entry\n",
    "# rather than creating a new one.\n",
    "test_sample = samples(sample.id, \"id\")[0]\n",
    "print(test_sample.sample_name)\n",
    "test_sample.sample_name = \"Terrible Rock\"\n",
    "test_sample.clean()\n",
    "test_sample.save()\n",
    "print(test_sample.sample_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.2: deleting model instances\n",
    "\n",
    "You can delete a database entry simply by calling its ```delete()``` method.\n",
    "Note that if other entries link to it -- for instance, a database of origin that is listed in many samples -- you won't be able to delete it while those other entries\n",
    "still exist in the database.\n",
    "\n",
    "### III.2.a: delete test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# it's probably better if we don't keep this Terrible Rock in the database \n",
    "# (see preceding section if you didn't make a Terrible Rock.)\n",
    "\n",
    "terrible_rock = samples(\"Terrible Rock\", \"sample_name\")[0]\n",
    "terrible_rock.delete()\n",
    "samples(\"Terrible Rock\", \"sample_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.3: bulk modification\n",
    "\n",
    "These techniques can be combined with standard Python control structures to\n",
    "change many items at once. Most of these examples are 'disarmed', with their\n",
    "```save``` or ```delete``` calls commented out. **Make sure you back the\n",
    "database up first if you arm and run them!** Running these without\n",
    "saving samples but leaving ```print()``` statements in acts as a 'dry run', and is\n",
    "very useful to verify that your changes are good before you commit them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.3.a: reprocess every sample in the database\n",
    "\n",
    "You might want to do this if you need to recalculate simulated spectra values\n",
    "because you've added new filtersets, or if you suspect that some malformed\n",
    "entries snuck in to the database and you'd like to reprocess entries one-by-one\n",
    "to find them.\n",
    "\n",
    "*Note: At current database size, assuming everything processes cleanly, this\n",
    "will probably take between half an hour and an hour and a half depending on\n",
    "operating environment (primarily single-core speed and secondarily\n",
    "disk throughput). You might want to add a progress timer or something.\n",
    "Also, this one is mostly harmless -- if everything\n",
    "is ok with a sample, it will just save it back to the database unchanged.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, sample in enumerate(samples()):\n",
    "    # good to know in case it hits something bad and crashes -- \n",
    "    # you have the name, id, and index (list position) of the sample to investigate\n",
    "    print(ix, sample.sample_name, sample.id) \n",
    "    sample.clean()\n",
    "    sample.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.3.b: find all samples without a sample name and assign placeholders\n",
    "\n",
    "Let's say some samples don't have a sample name, either because of an accidental omission in the source database or an unusual metadata convention that wasn't caught when scraping / importing from that database. Let's look at all those samples and assign sample names from their composition values. Also, let's check what databases they're from so that we can diagnose that problem.\n",
    "\n",
    "Using the `bulk_update` function makes this much much faster -- but note that when you do that, it only modifies\n",
    "metadata, so if something else about the sample is mangled, it's better to call `save`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unnamed = []\n",
    "for sample in samples(\"\", querytype=\"iexact\"):\n",
    "    if sample.composition:\n",
    "        sample.sample_name = sample.composition\n",
    "        print(sample.sample_name + \" from \" + sample.origin.name)\n",
    "        unnamed.append(sample)\n",
    "Sample.objects.bulk_update(unnamed, ['sample_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.3.c: standardize unit names across the database\n",
    "\n",
    "The are some samples in the database that give grain size in micrometers as 'um' and some that give it as 'microns'. Also, some samples have spaces between SI unit abbreviations and numerals, and some don't. Let's say you'd like to regularize this to always use 'um' for micrometers and also not have spaces between numerals and abbreviations. This replacement may be too crude, so we include print statements to see if it's good or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_si_units = are_in([\"cm\", \"mm\", \"nm\", \"um\"], or_)\n",
    "standardized = []\n",
    "for sample in samples():\n",
    "    if not sample.grain_size:\n",
    "        continue # don't bother doing anything if there's no grain size metadata\n",
    "    \n",
    "    replaced = sample.grain_size.replace(\"microns\", \"um\")\n",
    "    # we don't want to remove spaces in phrases that don't contain si units\n",
    "    if has_si_units(sample.grain_size):\n",
    "        replaced = replaced.strip().replace(\" \", \"\")\n",
    "    # nothing happened, move on\n",
    "    if sample.grain_size == replaced:\n",
    "        continue\n",
    "    print(f\"original: {sample.grain_size}; reformatted: {replaced}\")\n",
    "    sample.grain_size = replaced\n",
    "    standardized.append(replaced)\n",
    "# if you're happy with everything:\n",
    "# Sample.objects.bulk_update(standardized, ['grain_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.3.d: mark every sample from a particular origin as released\n",
    "\n",
    "By default, VISOR treats new samples as private -- specifically, their \"released\" field is set to ```False```, and only users logged in as admins can view them. You might use a command like the following when you're done QAing all the samples from a new source and you'd like to release them all to the public -- or if you've reingested all the samples for some reason and immediately want to mark them as released."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wwu = samples(\"Western Washington\", \"origin__name\")\n",
    "unreleased = []\n",
    "for sample in wwu_samples:\n",
    "    sample.released = False\n",
    "    unreleased.append(sample)\n",
    "Sample.objects.bulk_update(unreleased, ['released'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.3.e: assign all samples listed in an external file to a custom library \n",
    "You've received suggestions, given as sample IDs, to add to a custom library, and you've compiled all of those suggestions into a text file with one sample on each line. This assigns every sample in that file to a custom library. The Library model is different from other models because it has a \"many-to-many\" relationship with samples. The method for associating a sample and a library is therefore a little different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the new library\n",
    "test_library = Library(name='test library')\n",
    "test_library.clean()\n",
    "test_library.save()\n",
    "\n",
    "# read file in and split it line-by-line into a list\n",
    "with open(\"tests/custom_ids.txt\") as library_entry_file:\n",
    "    library_ids = library_entry_file.read().splitlines()\n",
    "# get all the samples matching these ids and squeeze them into a single queryset \n",
    "library_samples = reduce(or_, [\n",
    "    samples(library_id, 'sample_id') for library_id in library_ids\n",
    "])\n",
    "\n",
    "# because a single sample can belong to many libraries, you can't add a sample\n",
    "# to a library through direct assignment. instead use the add method of sample.libraries:\n",
    "for sample in library_samples:\n",
    "    sample.libraries.add(test_library)\n",
    "\n",
    "# you can check samples in a library using the Library.sample_set.all method:\n",
    "\n",
    "print(test_library.sample_set.all())\n",
    "\n",
    "# clean up\n",
    "for sample in library_samples:\n",
    "    sample.libraries.remove(test_library)\n",
    "    sample.clean()\n",
    "    sample.save(convolve=False)\n",
    "\n",
    "test_library.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.3.f: add an image to a sample\n",
    "\n",
    "You can assign either a path to a JPEG file or in-memory image data (as a PIL.Image) object to a sample's 'image' field. When you save it, it gets moved to the VISOR image directory, thumbnailed, and linked to the database entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a test version of a sample\n",
    "sample = samples()[0]\n",
    "sample.id = 100000000000\n",
    "sample.sample_id = \"TEST\"\n",
    "sample.image = 'tests/test_rock.jpg'\n",
    "sample.clean()\n",
    "sample.save()\n",
    "print(sample.image)\n",
    "sample.get_image() # displays image in jupyter. clunky but fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean this test sample up \n",
    "# Note / TODO: we don't currently delete images along with samples. this is a \n",
    "# way to do so, but I'm probably going to add some sort of automatic cleanup after \n",
    "# we're more certain about how we're going to use images in the application. --michael\n",
    "\n",
    "# delete image\n",
    "os.remove(settings.SAMPLE_IMAGE_PATH + \"/\" + sample.image)\n",
    "# delete sample database entry\n",
    "sample.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV: adding new data\n",
    "----\n",
    "## IV.1: importing sample files\n",
    "Sample data can be imported using the upload interface in the application, or pasted in / modified using the admin console. However, the same underlying functions can also be called from admin console or notebook. Also, there are some functions that can *only* be accessed from admin console / shell / notebook. In particular, there are 'safety' features in the upload interface that won't allow you to re-upload samples with identical sample_id, which means that you can't *update* a sample using the upload interface. (You can always activate these protections by passing ```uploaded=True``` to ```Sample.save()```.) \n",
    "\n",
    "### IV.1.a: import a single sample file\n",
    "```ingest_sample_csv``` is the primary function used to import files into the database. Passing it the name of a file in the Western Mars Lab spectrum CSV format will return a ```dict``` containing a Sample instance that can then be saved in the database, the filename of the ingested file, and warnings and errors if applicable. Also remember that, by default, every sample is imported 'unreleased', only visible to users with admin permissions. Set a sample's 'released' field to ```True``` to make it immediately visible.\n",
    "\n",
    "'warnings' mostly includes things the ingestion function did that changed some \n",
    "values in the imported sample.\n",
    "\n",
    "If there's anything in 'errors', the input file isn't valid and needs to be altered\n",
    "in order to go in the database. Lots of special cases are covered in the ingestion code and it should give useful error messages for many different sorts of problems. \n",
    "\n",
    "*Note: Please let me know if there's another case you need verbose feedback about. --michael*\n",
    "\n",
    "*Note / TODO: we don't have an updated version of the format standard description yet, but the format remains similar, so many examples are available*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a good sample file\n",
    "\n",
    "ingest_dict = ingest_sample_csv(\n",
    "    'tests/single_column_test.csv'\n",
    ")\n",
    "# 'filename' and 'warning' are also placed in the sample.filename and \n",
    "# sample.import_notes fields respectively. they are used internally,\n",
    "# but are available in the return because it can be useful to \n",
    "# print or manipulate them separately, mostly for error-checking purposes. \n",
    "print(ingest_dict)\n",
    "sample = ingest_dict['sample']\n",
    "sample.clean()\n",
    "sample.save()\n",
    "# test re-save with upload / anti-dupe protections\n",
    "try:\n",
    "    sample.save(uploaded=True)\n",
    "except ValueError as dupe_error:\n",
    "    print(dupe_error)\n",
    "print(samples(\"TEST\", \"sample_name\"))\n",
    "# delete this test sample\n",
    "# sample.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are bad sample files\n",
    "bad_ingest_dict_1 = ingest_sample_csv(\n",
    "    'tests/single_column_test_error_1.csv'\n",
    ")\n",
    "bad_ingest_dict_2 = ingest_sample_csv(\n",
    "    'tests/single_column_test_error_2.csv'\n",
    ")\n",
    "bad_ingest_dict_1['errors'], bad_ingest_dict_2['errors']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.1.b import a \"multisample\" file\n",
    "```ingest_sample_csv``` can also ingest files containing multiple wavelength / reflectance columns. It splits these into a list of Sample objects and increments their Sample IDs to distinguish the columns from one another.\n",
    "\n",
    "*Note / TODO: I don't know much about how the Western Mars Lab uses this format internally, so I don't know how to contextualize it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multisample = ingest_sample_csv('tests/multicolumn_test.csv')\n",
    "print(multisample['warnings'])\n",
    "for sample in multisample['sample']:\n",
    "    print(sample.sample_id)\n",
    "    sample.clean()\n",
    "#     sample.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.1.c: import all CSV files in a directory\n",
    "```ingest_sample_csv``` is happy to be used inside Python control structures. This can be used as an alternate way to ingest samples in bulk. *Note that this following cell doesn't handle multisamples, but can easily be extended to do so.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_directory = 'csv_library/'\n",
    "print(len(os.listdir(csv_directory)))\n",
    "for ix, file in enumerate(os.listdir(csv_directory)):\n",
    "    if not file.endswith('.csv'):\n",
    "        continue    \n",
    "    ingest_dict = ingest_sample_csv(csv_directory + file)\n",
    "    if ingest_dict[\"errors\"]:\n",
    "        print(\"This sample has problems!\")\n",
    "        print(ingest_dict[\"errors\"])\n",
    "        print(ix, file)\n",
    "        continue\n",
    "    ingest_dict[\"sample\"].clean()\n",
    "    ingest_dict[\"sample\"].save()\n",
    "    print(ingest_dict[\"sample\"].id, ix, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV.2: filtersets\n",
    "### IV.2.a: creating a new filterset\n",
    "\n",
    "VISOR does not offer a fully automated method for ingesting a new\n",
    "```FilterSet```. This is because response curves for different instruments\n",
    "are given in a wide variety of formats, and we do not have a standardized\n",
    "format for representing them. *Note / TODO: we could develop one, though!\n",
    "--michael* However, it offers a convenience function, ```make_filterset```, designed to help the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by defining a pandas dataframe with two columns:\n",
    "# filter name and canonical center wavelength.\n",
    "# in this case, we have high-resolution response curves for the instrument --\n",
    "# these center wavelengths are simply the points at which we \n",
    "# convolve the instrument response curve with the lab spectra.\n",
    "\n",
    "# note that we're following the MERTools convention here --\n",
    "# scale to the left eye, don't duplicate filters with centers\n",
    "# within 5nm.\n",
    "# also note that we don't currently have curves for L7/R7, but this doesn't\n",
    "# particularly matter because we're probably not going to be looking at\n",
    "# geological features through them.\n",
    "ZCAM_FREQS = pd.DataFrame(\n",
    "    {\n",
    "        \"filter\": [\n",
    "            \"L0R\", \"L0G\", \"L0B\", \"L1\",\n",
    "            \"L2\", \"L3\", \"L4\", \"L5\",\n",
    "            \"L6\", \"R2\", \"R3\",\n",
    "            \"R4\", \"R5\", \"R6\",\n",
    "        ],\n",
    "        \"wavelength\": [\n",
    "            630, 544, 480, 800,\n",
    "            754, 677, 605, 528,\n",
    "            442, 866, 910, \n",
    "            939, 978, 1022\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "# 5nm-spaced wavelength bins for numerical integration, going\n",
    "# from the bottom to the top of the wavelength ranges referenced\n",
    "# in the ZCAM filter response files\n",
    "ZCAM_BINS = np.arange(300,1105,5)\n",
    "# where are we storing the filter response curve files?\n",
    "ZCAM_FILTER_PATH = 'filters/mastcam_z/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read filter definition files into a dictionary of pandas dataframes\n",
    "# giving wavelength vs. responsivity for each filter.\n",
    "# if we didn't have these files, and only had, say, a specification \n",
    "# for band center + FWHM of each filter, we would numerically generate \n",
    "# responsivity curves for each filter at this step.\n",
    "# note that we load all the curves in, but when we generate simulated\n",
    "# spectra, we only actually use the filters referenced in ZCAM_FREQS,\n",
    "# which becomes the filter_frequencies field of the FilterSet object.\n",
    "filter_files = [file for file in os.listdir(ZCAM_FILTER_PATH)]\n",
    "filters = {}\n",
    "for filter_file in filter_files:\n",
    "    filter_name = re.search(r\"(L|R).{1,2}(?=\\.)\", filter_file).group(0)\n",
    "    filters[filter_name] = pd.read_csv(\n",
    "        ZCAM_FILTER_PATH + filter_file,\n",
    "        names=['wavelength', 'responsivity']\n",
    "    )\n",
    "print(filters.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_filterset is a convenience function that takes the values defined\n",
    "# above, interpolates each filter response curve to the array of shared bins,\n",
    "# power-normalizes each of these interpolated curves, and builds a FilterSet object. \n",
    "# interpolation and normalization are helpful to get consistent results from \n",
    "# numerical integration and present users with apples-to-apples comparisons on \n",
    "# graphs; any consequent loss of precision is not meaningful in this application\n",
    "# (and possibly not at all).\n",
    "# we also pass it a path to a CSV file of solar spectra that will be used to simulate \n",
    "# observations with solar illumination.\n",
    "\n",
    "zcam_filterset = make_filterset(\n",
    "    'Mars-2020 Mast Camera Zoom (Mastcam-Z)',\n",
    "    filters,\n",
    "    ZCAM_BINS,\n",
    "    ZCAM_FREQS,\n",
    "    \"filters/sun_input.csv\"\n",
    ")\n",
    "# set short name and reference URL\n",
    "zcam_filterset.short_name = 'Mastcam-Z'\n",
    "zcam_filterset.url = \"https://www.hou.usra.edu/meetings/lpsc2020/pdf/2312.pdf\"\n",
    "\n",
    "zcam_filterset.clean()\n",
    "zcam_filterset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAYBE: IV.3: creating new entries manually\n",
    "\n",
    "Automatic import functions aren't the only way to produce new database entries. You can also manually generate them. This will often be more awkward than using import functions, but is preferable or necessary in some cases.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in Sample.objects.all():\n",
    "    sample.released = True\n",
    "    sample.clean()\n",
    "    sample.save(convolve=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
