{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VNIRSD admin programming manual and  recipes\n",
    "----\n",
    "###  michael st. clair \n",
    "### v0.1 -- 02-24-2021\n",
    "\n",
    "This Jupyter Notebook discusses the structure of the VNIRSD backend and methods for modifying its contents using Python scripts. The code cells in this notebook\n",
    "are intended both as illustrative examples and as useful 'recipes' that\n",
    "can be minimally modified and incorporated into other notebooks or scripts to perform\n",
    "administrative tasks.\n",
    "\n",
    "*This is a preliminary version of this document intended for internal\n",
    "operations. Please do not publicly distribute.*\n",
    "\n",
    "### usage notes\n",
    "----\n",
    "* This notebook should always be launched using ```python manage.py shell_plus --notebook```. Otherwise, django won't get to run its setup scripts and you will get errors when you try to import models or alter the database. \n",
    "* many of these cells create huge amounts of output. to shrink this if you're tired of looking at it, double-click on the gutter to the left of the cell. to totally erase it, go up to the 'cell' menu and choose 'current outputs -> clear', or 'all outputs' -> clear to get rid of output for every cell.\n",
    "* this notebook assumes that it's being run in the root directory of the application. If you move it somewhere else or use the code snippets in files located in other places, you may have to adjust paths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports\n",
    "----\n",
    "Run this next cell if you want the code to function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from functools import partial\n",
    "import json\n",
    "from operator import or_\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "\n",
    "from django.conf import settings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from recipes import samples\n",
    "from vnirsd.dj_utils import are_in, djget, eta, fields, ingest_sample_csv\n",
    "from vnirsd.models import Database, Sample\n",
    "from vnirsd.spectral import make_filterset\n",
    "\n",
    "# the examples in this notebook don't do risky async stuff to the database.\n",
    "# they all work the exact same way on the backend as the admin console. however, \n",
    "# ipython/jupyter wraps itself in an event loop that looks scary to django. this \n",
    "# environment variable tells django to calm down about it.\n",
    "os.environ[\"DJANGO_ALLOW_ASYNC_UNSAFE\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I: database structure\n",
    "----\n",
    "\n",
    "## I.1: basics and location\n",
    "\n",
    "The VNIRSD is backed by a SQLite database. This database is entirely contained\n",
    "in one file: db.sqlite3. **Keep several backups of this file outside of the\n",
    "working tree of the application.** This lets you freely experiment with the\n",
    "database. If you do anything horrible to it, you can immediately repair it by\n",
    "overwriting the file in the application directory with one of these backups. The only entry-specific items that are not stored in this database are image files (links to the images are stored, but not binary image content itself, because this kind of\n",
    "thing is generally a bad idea; filesystems are better than databases at storing\n",
    "files).\n",
    "\n",
    "### II.1.a: working live or offline\n",
    "\n",
    "Because it's easy to copy the single database file, you can also work in a separate development directory, or on a totally different machine with a version of the software running locally. On the other hand, because it's easy to fix mistakes, it's also pretty safe to work on the live version. You can even swap the database file out while the application is running. Users will only notice if they make queries while the file is in the middle of being overwritten. \n",
    "\n",
    "## I.2: django and models\n",
    "\n",
    "The VNIRSD primarily uses the Python framework Django to interact with the\n",
    "database. Django abstracts SQL tables as instances of the class ```Model```.\n",
    "There are five important models / tables in the VNIRSD proper:\n",
    "\n",
    "* ```Sample``` (individual samples)\n",
    "* ```Database``` (origin databases, like ASTER/ECOSTRESS)\n",
    "* ```FilterSet``` (definitions for sets of filters, like Mastcam-Z's, used\n",
    "    for generating simulated reflectance curves)\n",
    "* ```Library``` (application- or team-specific groups of samples, and maybe\n",
    "    other things later -- this is fully functional, but not currently populated)\n",
    "* ```SampleType``` (top-level physical categories of sample, like minerals or\n",
    "    coatings -- this is again fully functional, but only skeletally populated)\n",
    "\n",
    "*Note: while you probably don't want to interact with them from the Python\n",
    "shell, admin models, including users and their access information, are also\n",
    "stored in the database. So, for instance, if you roll back to an earlier\n",
    "version of the database after changing a user's password but before making a\n",
    "new database backup, that password will be reset to the earlier version.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. searching the database\n",
    "----\n",
    "\n",
    "## II.1: searching the database\n",
    "\n",
    "Django uses ```QuerySet``` objects and methods to interact with ```Model```\n",
    "objects. These are powerful, but use a custom syntax that combines SQL queries and Python. This syntax is sometimes awkward and rarely looks like idiomatic Python. This section discusses ways to skirt the queryset syntax to more easily select objects in the database.\n",
    "\n",
    "### II.1.a: custom search functions\n",
    "\n",
    "The next cell defines a simple search function ```samples``` that looks for\n",
    "samples that contain a particular value anywhere in a particular field, case-\n",
    "insensitive.\n",
    " \n",
    "*Note: ```samples``` is also included in the recipes.py module, but\n",
    "manipulating the code in the cell below will allow you to define different versions of\n",
    "it.*\n",
    "\n",
    "The syntax is simply: ```samples(value, field)```; it returns a ```QuerySet```\n",
    "object (which can mostly be treated as a ```list``` with special\n",
    "functionality) of all samples containing that value in that field.\n",
    "\n",
    "Other useful values for ```querytype``` in cousins of ```samples``` include\n",
    "'lt' or 'gt' (less/greater than) or 'iexact' (exact match). dropping the\n",
    "leading 'i' makes the search case-sensitive.\n",
    "\n",
    "The names the VNIRSD prints for Sample objects in Python shell / notebook are formatted like this:\n",
    "\n",
    "```sample name + _ + sample id (in database of origin) + _ + database-of-origin short name```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# define partially-evaluated convenience function\n",
    "get_contains = partial(\n",
    "    djget, \n",
    "    model=Sample, \n",
    "    value = \"\",\n",
    "    # the field value is model-specific! you can omit it if you don't want to\n",
    "    # use the shortened call types discussed in II.1.c\n",
    "    field = \"sample_name\", \n",
    "    querytype='icontains'\n",
    ")\n",
    "# reorder arguments to prevent collisions\n",
    "samples = eta(get_contains, \"value\", \"field\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1.b: fetch all hematites in the database and look at 5 of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: the vanilla django equivalent to the next line is: \n",
    "# hematites = Sample.objects.filter(sample_name__icontains='hematite')\n",
    "hematites = samples('hematite', 'sample_name')\n",
    "random.choices(hematites, k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.1.c: check total number of samples, or of a subset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# samples() looks in the sample_name field by default.\n",
    "# called with no arguments, it returns all values in the model.\n",
    "# note: the vanilla django way to do that is to call Sample.objects.all().\n",
    "\n",
    "len(samples(\"smectite\")), len(samples()), len(Sample.objects.all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.2: fields and values of models\n",
    "\n",
    "There are lots of ways to get fields and field values from the database. See the next few cells for some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2.a: get every field of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fields(Sample), fields(Database)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2.b: get values of a particular field from instances of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_sample = random.choice(samples())\n",
    "print(random_sample.sample_name)\n",
    "smectites = samples(\"smectite\")\n",
    "print([\n",
    "    sample.id for sample in smectites\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.2.c: get unique values of a field, ordered alphabetically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "names = [\n",
    "    name_list[0] for name_list in\n",
    "    set(samples().values_list('grain_size'))\n",
    "]\n",
    "names.sort()\n",
    "names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## II.3 related model fields\n",
    "\n",
    "Accessing fields of related objects from a different model is done by, depending on context:\n",
    "* using chained accessors (like: ```model.other_model.field```)\n",
    "* separating the related field name and the field you want to access on the\n",
    "    other model by a double underscore (like: ```sc(value, \"other_model__field\")```)\n",
    "\n",
    "### II.3.a: learn about a sample's database of origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_sample = random.choice(samples())\n",
    "print(random_sample.origin.name) # full name of that sample's database of origin\n",
    "print(random_sample.origin.url) # url for that sample's database of origin\n",
    "# is that sample in the group of all samples whose databases of origin have that \n",
    "# full name? (hopefully yes, or something is very wrong) \n",
    "print(random_sample in samples(random_sample.origin.name, \"origin__name\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II.4 interpreting Sample model fields\n",
    "\n",
    "There are a *lot* of fields on the ```Sample``` model, and most of them are \n",
    "empty for most samples. This is because the table is intended to support\n",
    "content ingested from a bunch of different databases,  each of which has its\n",
    "own metadata standard. So, for instance, while we'd like to retain information\n",
    "about resolution if it's available in an input database, most of our input\n",
    "databases don't provide\n",
    "resolution values in their metadata. The fields you can expect to be on every\n",
    "or almost every sample are:\n",
    "* sample_name (Name of the sample from the original database, like \"Talc\")\n",
    "* sample_id (ID of the sample from the original database, retained for\n",
    "traceability)\n",
    "* id (unique ID number in VNIRSD, also known as a database primary key or PK)\n",
    "    * bear in mind that because a primary key is how a database distinguishes\n",
    "    objects, changing a sample's id field makes it a whole new entry\n",
    "* date_added (last modification date of the sample)\n",
    "* min_reflectance (minimum wavelength in the reflectance array)\n",
    "* max_reflectance (maximum wavelength in the reflectance array)\n",
    "* origin (database of origin -- this is an instance of the ```Database```\n",
    "    model)\n",
    "* released (has the sample been released to the public?)\n",
    "* reflectance (reflectance array flattened into a simple string) \n",
    "* simulated_spectra (dictionary of ```pandas DataFrames``` giving simulated\n",
    "    reflectance arrays flattened into a json string)\n",
    "\n",
    "### II.4.a: get a random sample and look at all its fields\n",
    "\n",
    "You can use the ```as_dict()``` method of a ```Sample``` object to get most\n",
    "things about it in a ```dict``` -- note that the flattened reflectance and\n",
    "simulated_spectra fields aren't very readable! See the next few cells for\n",
    " ways to interpret them as ```numpy``` arrays and ```pandas``` dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_sample = random.choice(samples())\n",
    "random_sample.as_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.4.b: look at properties of that sample's reflectance data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "reflectance = np.array(literal_eval(random_sample.reflectance))\n",
    "print(reflectance[0:10, 0]) # first 10 wavelength values of spectrum\n",
    "print(np.median(reflectance,0))  # median wavelength and reflectance of spectrum \n",
    "print(reflectance[:,1].mean()) # mean reflectance of spectrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II.4.c: look at a simulated spectrum for that sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sim_landsat = pd.DataFrame(\n",
    "    json.loads(literal_eval(random_sample.simulated_spectra)['LANDSAT 8 OLI'])\n",
    ")\n",
    "sim_landsat # dataframe containing simulated values for Landsat 8 OLI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# III: manipulating database entries\n",
    "----\n",
    "## III.1: field assignment and model entry updates\n",
    "\n",
    "Similar methods can be used to modify entries in the database. The easiest way\n",
    "is to assign values directly to fields of a model instance. This is useful if\n",
    "you want to quickly modify items without using the admin console.\n",
    "**Important:** updating the fields of a model instance in memory **does not**\n",
    "automatically change the corresponding instance in the database. After\n",
    "modifying a model instance, calling its ```clean()``` and ```save()``` methods\n",
    "will validate its changed data and record the updated version in the database.\n",
    "Some other stuff only happens after you call ```save()```, generally things that require comparisons with other values in the database. For instance,\n",
    "simulated spectra are generated at that point for samples, and if a model\n",
    "instance doesn't have an id / primary key, it gets assigned one.\n",
    "\n",
    "### III.1.a: change an in-memory sample without saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample = samples()[0]\n",
    "sample.sample_name = sample.sample_name + \"_TEST\"\n",
    "print(sample.sample_name) # great! working great, right? the sample is updated!\n",
    "sample = samples()[0]\n",
    "print(sample.sample_name) # aww...no, the sample wasn't updated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.1.b: make a test version of a sample and save it in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample = samples()[0]\n",
    "# remember that changing id / primary key makes something a \"new\" object from the \n",
    "# database's perspective; delete id so we don't overwrite the real sample\n",
    "sample.sample_name = sample.sample_name + \"_TEST\"\n",
    "sample.id = None \n",
    "sample.released = False # don't show visitors our silly test sample\n",
    "sample.clean() # validate sample fields\n",
    "sample.save() # save it in the database\n",
    "# is it there, and different from the original? hopefully.\n",
    "samples()[0], samples(sample.id, \"id\")[0] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.2.c: modify and save the test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# saving a model instance _without_ changing its id modifies the existing entry\n",
    "# rather than creating a new one.\n",
    "test_sample = samples(10000000, \"id\")[0]\n",
    "print(test_sample.sample_name)\n",
    "test_sample.sample_name = \"Terrible Rock\"\n",
    "test_sample.clean()\n",
    "test_sample.save()\n",
    "print(test_sample.sample_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.2: deleting model instances\n",
    "\n",
    "You can delete a database entry simply by calling its ```delete()``` method.\n",
    "Note that if other entries link to it -- for instance, the database of origin\n",
    "for many samples -- you won't be able to delete it while those other entries\n",
    "still exist in the database.\n",
    "\n",
    "### III.2.a: delete test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# it's probably better if we don't keep this Terrible Rock in the database \n",
    "# (see preceding section if you didn't make a Terrible Rock.)\n",
    "\n",
    "terrible_rock = samples(\"Terrible Rock\", \"sample_name\")[0]\n",
    "terrible_rock.delete()\n",
    "samples(\"Terrible Rock\", \"sample_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III.3: bulk modification\n",
    "\n",
    "These techniques can be combined with standard python control structures to\n",
    "change many items at once. Most of these examples are 'disarmed', with their\n",
    "```save``` or ```delete``` calls commented out. **Make sure you back the\n",
    "database up first if you arm and run them!** Running these without\n",
    "saving samples but leaving print() statements in acts as a 'dry run', and is\n",
    "very useful to verify that your changes are good before you commit them.\n",
    "\n",
    "### III.3.a: delete everything\n",
    "\n",
    "Perhaps you want to delete every sample from the database, but you don't want\n",
    "to dump the entire file -- for instance, you don't want to generate a new\n",
    "password for every user. Here's a way to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# back up first. really. not joking!\n",
    "\n",
    "# for sample in samples():\n",
    "#     sample.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.3.b: reprocess every sample in the database\n",
    "\n",
    "You might want to do this if you need to recalculate simulated spectra values\n",
    "because you've added new filtersets, or if you suspect that some malformed\n",
    "entries snuck in to the database and you'd like to reprocess entries one-by-one\n",
    "to find them.\n",
    "\n",
    "*Note: At current database size, assuming everything processes cleanly, this\n",
    "will probably take between half an hour and an hour and a half depending on\n",
    "operating environment (primarily single-core speed and secondarily\n",
    "disk throughput). You might want to add a progress timer or something.\n",
    "Also, this one is mostly harmless -- if everything\n",
    "is ok with a sample, it will just save it back to the database unchanged.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ix, sample in enumerate(samples()):\n",
    "    # good to know in case it hits something bad and crashes -- \n",
    "    # you have the name, id, and index (list position) of the sample to investigate\n",
    "    print(ix, sample.sample_name, sample.id) \n",
    "    sample.clean()\n",
    "    sample.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.3.c: find all samples without a sample name and assign placeholders\n",
    "\n",
    "Let's say some samples don't have a sample name, either because of an accidental omission in the source database or an unusual metadata naming convention that wasn't caught when scraping / importing from that database. Let's look at all those samples and assign their composition values as sample names; also, let's check what databases they're from so that we can go diagnose that problem.\n",
    "\n",
    "The slowest thing about saving samples is convolving them with all the filtersets. Passing the parameter ```convolve=False``` to the ```save``` method of ```Sample``` prevents reconvolving samples and makes changes like this faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in samples(\"\", querytype=\"iexact\"):\n",
    "    if sample.composition:\n",
    "        sample.sample_name = sample.composition\n",
    "    print(sample.sample_name + \" from \" + sample.origin.name)\n",
    "    sample.clean()\n",
    "    sample.save(convolve=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.3.d: standardize unit names across the database\n",
    "\n",
    "The are some samples in the database that give grain size in micrometers as 'um' and some that give it as 'microns'. Also, some samples have spaces between SI unit abbreviations and numerals, and some don't. Let's say you'd like to regularize this to always use 'um' for micrometers and also not have spaces between numerals and abbreviations. This replacement may be too crude, so we include print statements to see if it's good or not. *(TODO: If it is, move on to regex, maybe or maybe not beyond the scope of this manual. -- michael)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_si_units = are_in([\"cm\", \"mm\", \"nm\", \"um\"], or_)\n",
    "for sample in samples():\n",
    "    if not sample.grain_size:\n",
    "        continue # don't bother doing anything if there's no grain size metadata\n",
    "    print(\"original\", sample.grain_size)\n",
    "    sample.grain_size = sample.grain_size.replace(\"microns\", \"um\")\n",
    "    # we don't want to remove spaces in phrases that don't contain si units\n",
    "    if has_si_units(sample.grain_size):\n",
    "        sample.grain_size = sample.grain_size.strip().replace(\" \", \"\")\n",
    "    print(\"reformatted\", sample.grain_size)\n",
    "    sample.clean()\n",
    "    # sample.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.3.e: mark every sample from a particular origin as released\n",
    "\n",
    "By default, the VNIRSD treats new samples as private -- specifically, their \"released\" field is set to ```False```, and only users logged in as admins can view them. You might use a command like the following when you're done QAing all the samples from a new source and you'd like to release them all to the public -- or if you've reingested all the samples for some reason and immediately want to mark them as released. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in samples(\"Western Washington\", \"origin__name\"):\n",
    "    sample.released = True\n",
    "    sample.clean()\n",
    "#     sample.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.4.f: assign all samples listed in an external file to a custom library [TODO: UNFINISHED]\n",
    "You've received suggestions, given as sample IDs, to add to a custom library, and you've compiled all of those suggestions into a text file with one sample on each line. This assigns every sample in that file to a custom library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tests/custom_library_entries.txt\") as library_entry_file:\n",
    "    library_entries = library_entry_file.readlines()\n",
    "for entry in library_entries:\n",
    "    sample = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.4.g: add an image to a sample\n",
    "\n",
    "You can assign either a path to a JPEG file or in-memory image data (as a PIL.Image) object to a sample's 'image' field. When you save it, it gets moved to the VNIRSD image directory, thumbnailed, and linked to the database entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a test version of a sample\n",
    "sample = samples()[0]\n",
    "sample.id = 100000000000\n",
    "sample.sample_id = \"TEST\"\n",
    "sample.image = 'tests/test_rock.jpg'\n",
    "sample.clean()\n",
    "sample.save()\n",
    "print(sample.image)\n",
    "sample.get_image() # displays image in jupyter. clunky but fine. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean this test sample up \n",
    "# Note / TODO: we don't currently delete images along with samples. this is a \n",
    "# way to do so, but I'm probably going to add some sort of automatic cleanup after \n",
    "# we're more certain about how we're going to use images in the application. --michael\n",
    "\n",
    "# delete image\n",
    "os.remove(settings.SAMPLE_IMAGE_PATH + \"/\" + sample.image)\n",
    "# delete sample database entry\n",
    "sample.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV: adding new data\n",
    "----\n",
    "## IV.1: importing sample files\n",
    "Sample data can be imported using the upload interface in the application, or pasted in / modified using the admin console. However, the same underlying functions can also be called from admin console or notebook. Also, there are some functions that can *only* be accessed from admin console / shell / notebook. In particular, there are 'safety' features in the upload interface that won't allow you to re-upload samples with identical sample_id, which means that you can't *update* a sample using the upload interface. (You can always activate these protections by passing ```uploaded=True``` to the ```Sample.save``` method.)\n",
    "\n",
    "### IV.1.a: import a single sample file\n",
    "```ingest_sample_csv``` is the primary function used to import files into the database. Passing it the name of a file in the Western Mars Lab spectrum CSV format will return a ```dict``` containing a Sample instance that can then be saved in the database, the filename of the ingested file, and warnings and errors if applicable. \n",
    "\n",
    "'warnings' mostly includes things the ingestion function did that changed some \n",
    "values in the imported sample.\n",
    "\n",
    "If there's anything in 'errors', the input file isn't valid and needs to be altered\n",
    "in order to go in the database. Lots of special cases are covered in the ingestion code and it should give useful error messages for many different sorts of problems. \n",
    "\n",
    "*Note: Please let me know if there's another case you need verbose feedback about. --michael*\n",
    "\n",
    "*Note / TODO: we don't have an updated version of the format standard description yet, but the format remains similar, so many examples are available*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a good sample file\n",
    "\n",
    "ingest_dict = ingest_sample_csv(\n",
    "    'tests/single_column_test.csv'\n",
    ")\n",
    "# 'filename' and 'warning' are also placed in the sample.filename and \n",
    "# sample.import_notes fields respectively. they are used internally,\n",
    "# but are available in the return because it can be useful to \n",
    "# print or manipulate them separately, mostly for error-checking purposes. \n",
    "print(ingest_dict)\n",
    "sample = ingest_dict['sample']\n",
    "sample.clean()\n",
    "sample.save()\n",
    "# test re-save with upload / anti-dupe protections\n",
    "try:\n",
    "    sample.save(uploaded=True)\n",
    "except ValueError as dupe_error:\n",
    "    print(dupe_error)\n",
    "print(samples(\"TEST\", \"sample_name\"))\n",
    "# delete this test sample\n",
    "sample.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are bad sample files\n",
    "bad_ingest_dict_1 = ingest_sample_csv(\n",
    "    'tests/single_column_test_error_1.csv'\n",
    ")\n",
    "bad_ingest_dict_2 = ingest_sample_csv(\n",
    "    'tests/single_column_test_error_2.csv'\n",
    ")\n",
    "bad_ingest_dict_1['errors'], bad_ingest_dict_2['errors']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.1.b import a \"multisample\" file\n",
    "```ingest_sample_csv``` can also ingest files containing multiple wavelength / reflectance columns. It splits these into a list of Sample objects and increments their Sample IDs to distinguish the columns from one another.\n",
    "\n",
    "*Note / TODO: I don't know much about how the Western Mars Lab uses this format internally, so I don't know how to contextualize it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multisample = ingest_sample_csv('tests/multicolumn_test.csv')\n",
    "print(multisample['warnings'])\n",
    "for sample in multisample['sample']:\n",
    "    print(sample.sample_id)\n",
    "    sample.clean()\n",
    "#     sample.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.1.c: import a zipped file containing multiple CSV files\n",
    "One way to ingest samples in bulk is to feed ```ingest_sample_csv``` a zipped file containing one or more CSV files, and optionally also JPEG image files sharing names with those CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingest_dict = ingest_sample_csv(\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.1.d: import all CSV files in a directory\n",
    "```ingest_sample_csv``` is happy to be used inside Python control structures. This can be used as an alternate way to ingest samples in bulk. *Note that this following cell doesn't handle multisamples, but can easily be extended to do so.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "csv_directory = '../wwu_spec_csvs/'\n",
    "print(len(os.listdir(csv_directory)))\n",
    "for ix, file in enumerate(os.listdir(csv_directory)):\n",
    "    if not file.endswith('.csv'):\n",
    "        continue    \n",
    "    ingest_dict = ingest_sample_csv(csv_directory + file)\n",
    "    if ingest_dict[\"errors\"]:\n",
    "        print(\"This sample has problems!\")\n",
    "        print(ingest_dict[\"errors\"])\n",
    "        print(ix, file)\n",
    "        continue\n",
    "    ingest_dict[\"sample\"].clean()\n",
    "    ingest_dict[\"sample\"].save()\n",
    "    print(ingest_dict[\"sample\"].id, ix, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV.2: filtersets\n",
    "### IV.2.a: creating a new filterset\n",
    "\n",
    "The VNIRSD does not offer a fully automated method for ingesting a new\n",
    "```FilterSet```. This is because response curves for different instruments\n",
    "are given in a wide variety of formats and we do not have a standardized\n",
    "format for representing them. *Note / TODO: we could develop one, though!\n",
    "--michael* However, it offers a convenience function, ```make_filterset```, designed to help the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by defining a pandas dataframe with two columns:\n",
    "# filter name and canonical center wavelength.\n",
    "# in this case, we have high-resolution response curves for the instrument --\n",
    "# these center wavelengths are simply the points at which we \n",
    "# convolve the instrument response curve with the lab spectra.\n",
    "\n",
    "# note that we're following the MERTools convention here --\n",
    "# scale to the left eye, don't duplicate filters with centers\n",
    "# within 5nm.\n",
    "# also note that we don't currently have curves for L7/R7.\n",
    "ZCAM_FREQS = pd.DataFrame(\n",
    "    {\n",
    "        \"filter\": [\n",
    "            \"L0R\", \"L0G\", \"L0B\", \"L1\",\n",
    "            \"L2\", \"L3\", \"L4\", \"L5\",\n",
    "            \"L6\", \"R2\", \"R3\",\n",
    "            \"R4\", \"R5\", \"R6\",\n",
    "        ],\n",
    "        \"wavelength\": [\n",
    "            630, 544, 482, 800,\n",
    "            754, 677, 605, 528,\n",
    "            442, 866, 910, \n",
    "            939, 978, 1022\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "# 5nm-spaced frequency bins for numerical integration, going\n",
    "# from the bottom to the top of the frequency ranges referenced\n",
    "# in the ZCAM filter response files\n",
    "ZCAM_BINS = np.arange(300,1105,5)\n",
    "# where are we storing the filter response curve files?\n",
    "ZCAM_FILTER_PATH = 'filters/mastcam_z/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read filter definition files into a dictionary of pandas dataframes\n",
    "# giving wavelength vs. responsivity for each filter.\n",
    "# if we didn't have these files, and only had, say, a specification \n",
    "# for band center + FWHM of each filter, we would numerically generate \n",
    "# responsivity curves for each filter at this step.\n",
    "# note that we load all the curves in, but when we generate simulated\n",
    "# spectra, we only actually use the filters referenced in ZCAM_FREQS,\n",
    "# which becomes the filter_frequencies field of the FilterSet object.\n",
    "filter_files = [file for file in os.listdir(ZCAM_FILTER_PATH)]\n",
    "filters = {}\n",
    "for filter_file in filter_files:\n",
    "    filter_name = re.search(r\"(L|R).{1,2}(?=\\.)\", filter_file).group(0)\n",
    "    filters[filter_name] = pd.read_csv(\n",
    "        ZCAM_FILTER_PATH + filter_file,\n",
    "        names=['wavelength', 'responsivity']\n",
    "    )\n",
    "print(filters.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_filterset is a convenience function that takes the values defined\n",
    "# above, interpolates each filter response curve to the array of shared bins,\n",
    "# power-normalizes each of these interpolated curves, and builds a FilterSet object. \n",
    "# interpolation and normalization are helpful to get consistent results from \n",
    "# numerical integration and present users with apples-to-apples comparisons on \n",
    "# graphs; any consequent loss of precision is not meaningful in this application\n",
    "# (and possibly not at all).\n",
    "# we also pass it a path to a CSV file of solar spectra that will be used to simulate \n",
    "# observations with solar illumination.\n",
    "\n",
    "zcam_filterset = make_filterset(\n",
    "    'Mars-2020 Mast Camera Zoom (Mastcam-Z)',\n",
    "    filters,\n",
    "    ZCAM_BINS,\n",
    "    ZCAM_FREQS,\n",
    "    \"filters/sun_input.csv\"\n",
    ")\n",
    "# set short name and reference URL\n",
    "zcam_filterset.short_name = 'Mastcam-Z'\n",
    "zcam_filterset.url = \"https://www.hou.usra.edu/meetings/lpsc2020/pdf/2312.pdf\"\n",
    "\n",
    "zcam_filterset.clean()\n",
    "zcam_filterset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAYBE: IV.3: creating new entries manually\n",
    "\n",
    "Automatic import functions aren't the only way to produce new database entries. You can also manually generate them. This will often be more awkward than using import functions, but is preferable or necessary in some cases.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
